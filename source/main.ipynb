{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "dataset_size = [\"small\", \"medium\", \"large\"][0]\n",
    "\n",
    "dataset_info = {\n",
    "    \"small\": {\n",
    "        \"dataset_name\": \"wine\",\n",
    "        \"class_name\": \"Class\",\n",
    "        \"drop_fields\": []\n",
    "    },\n",
    "    \"medium\": {\n",
    "        \"dataset_name\": \"breast-cancer-wisconsin\",\n",
    "        \"class_name\": \"Class\",\n",
    "        \"drop_fields\": [\"Sample code number\"]\n",
    "    },\n",
    "    \"large\": {\n",
    "        \"dataset_name\": \"agaricus-lepiota\",\n",
    "        \"class_name\": \"Class\",\n",
    "        \"drop_fields\": []\n",
    "    },\n",
    "}\n",
    "\n",
    "dataset_name = dataset_info[dataset_size][\"dataset_name\"]\n",
    "class_name = dataset_info[dataset_size][\"class_name\"]\n",
    "drop_fields = dataset_info[dataset_size][\"drop_fields\"]\n",
    "\n",
    "df = pd.read_csv('../data/' + dataset_name + \".csv\")\n",
    "df = df.drop(drop_fields, axis=1)\n",
    "df = df.iloc[np.random.permutation(len(df))]\n",
    "\n",
    "if dataset_name == \"breast-cancer-wisconsin\":\n",
    "    df[class_name].replace({2: 0, 4: 1}, inplace=True)\n",
    "    \n",
    "if dataset_name == \"agaricus-lepiota\":\n",
    "    df[class_name].replace({'p': 1, 'e': 0}, inplace=True)\n",
    "\n",
    "n_cut = int(0.8*len(df))\n",
    "df_trn = df[:n_cut]\n",
    "df_tst = df[n_cut:]\n",
    "\n",
    "X_trn = df_trn.drop(class_name, axis=1)\n",
    "y_trn = df_trn[class_name]\n",
    "\n",
    "X_tst = df_tst.drop(class_name, axis=1)\n",
    "y_tst = df_tst[class_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RandomForest_df import RandomForest_df\n",
    "\n",
    "M = X_trn.shape[1]\n",
    "CV_dict_params = {'NT': [1, 10, 25, 50, 75, 100],\n",
    "                  'F': sorted(list(set([1, 3, int(np.log2(M + 1)), int(np.sqrt(M))])))\n",
    "                  }\n",
    "\n",
    "best_F, best_NT, best_score = None, None, -1\n",
    "all_metrics_CV = []\n",
    "\n",
    "for F_ in CV_dict_params['F']:\n",
    "    for NT_ in CV_dict_params['NT']:\n",
    "        all_scores = np.zeros(5)\n",
    "        N = len(X_trn)\n",
    "        for run in range(5):\n",
    "            \n",
    "            ind_tst_ = np.full(len(X_trn), False)\n",
    "            ind_tst_[int(N*run/5):int(N*(run+1)/5)] = True\n",
    "            \n",
    "            X_trn_, y_trn_ = X_trn[ind_tst_], y_trn[ind_tst_]\n",
    "            X_tst_, y_tst_ = X_trn[~ind_tst_], y_trn[~ind_tst_]\n",
    "            \n",
    "            clf = RandomForest_df(NT=NT_, F=F_)\n",
    "            clf.fit(X_trn_, y_trn_)\n",
    "            all_scores[run] = clf.score(X_tst_, y_tst_)\n",
    "            \n",
    "        score_ = all_scores.mean()\n",
    "        all_metrics_CV.append(score_)\n",
    "        \n",
    "        if score_ > best_score:\n",
    "            best_F, best_NT, best_score = F_, NT_, score_\n",
    "        print(f'(F, NT) = {(F_, NT_)} \\t--> \\t F1-Score = {round(score_, 3)}')\n",
    "        \n",
    "all_metrics_CV = np.array(all_metrics_CV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best Parameters (F, NT): {(best_F, best_NT)}')\n",
    "best_RF = RandomForest_df(NT=best_NT, F=best_F)\n",
    "best_RF.fit(X_trn, y_trn, verbose=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trn_hat = best_RF.predict(X_trn)\n",
    "y_tst_hat = best_RF.predict(X_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy (test): {round(accuracy_score(y_tst.to_numpy(), y_tst_hat), 3)}')\n",
    "print(f'Precision (test): {round(precision_score(y_tst.to_numpy(), y_tst_hat), 3)}')\n",
    "print(f'Recall (test): {round(recall_score(y_tst.to_numpy(), y_tst_hat), 3)}')\n",
    "print(f'F1 Score (test): {round(f1_score(y_tst.to_numpy(), y_tst_hat), 3)}')\n",
    "print('-'*15)\n",
    "print(f'Accuracy (train): {round(accuracy_score(y_trn.to_numpy(), y_trn_hat), 3)}')\n",
    "print(f'Precision (train): {round(precision_score(y_trn.to_numpy(), y_trn_hat), 3)}')\n",
    "print(f'Recall (train): {round(recall_score(y_trn.to_numpy(), y_trn_hat), 3)}')\n",
    "print(f'F1 Score (train): {round(f1_score(y_trn.to_numpy(), y_trn_hat), 3)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_importances = []\n",
    "all_metrics = []\n",
    "\n",
    "for F_ in CV_dict_params['F']:\n",
    "    for NT_ in CV_dict_params['NT']:\n",
    "        clf = RandomForest_df(NT=NT_, F=F_)\n",
    "        print(f'(F, NT) = {(F_, NT_)}')\n",
    "        clf.fit(X_trn, y_trn, verbose=3)\n",
    "        all_importances.append(np.flip(np.argsort(clf.importance)))\n",
    "        y_tst_hat = clf.predict(X_tst)     \n",
    "        \n",
    "        acc = accuracy_score(y_tst.to_numpy(), y_tst_hat)\n",
    "        prec = precision_score(y_tst.to_numpy(), y_tst_hat)\n",
    "        rec = recall_score(y_tst.to_numpy(), y_tst_hat)\n",
    "        f1_ = f1_score(y_tst.to_numpy(), y_tst_hat)\n",
    "           \n",
    "        print(f'Accuracy (test): {round(acc, 3)}')\n",
    "        print(f'Precision (test): {round(prec, 3)}')\n",
    "        print(f'Recall (test): {round(rec, 3)}')\n",
    "        print(f'F1 Score (test): {round(f1_, 3)}')\n",
    "        print('-'*15)\n",
    "        \n",
    "        all_metrics.append([acc, prec, rec, f1_])\n",
    "        \n",
    "all_importances = np.array(all_importances)\n",
    "all_metrics = np.array(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_metrics.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_metrics_CV.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.print_latex import print_table\n",
    "\n",
    "# print(\"IMPORTANCES\")\n",
    "# print_table(all_importances.T)\n",
    "# print(\"-\"*15)\n",
    "\n",
    "# print(\"ACC - PRECISION - RECALL - F1 (TEST)\")\n",
    "# print_table(all_metrics.T)\n",
    "# print(\"-\"*15)\n",
    "\n",
    "# print(\"F1 (CV)\")\n",
    "# print_table(all_metrics_CV)\n",
    "# print(\"-\"*15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sys import modules\n",
    "# del modules[\"utils.print_latex\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DecisionForest_df import DecisionForest_df\n",
    "\n",
    "M = X_trn.shape[1]\n",
    "CV_dict_params = {'NT': [1, 10, 25, 50, 75, 100],\n",
    "                  'F': sorted(list(set([int(M/4), int(M/2), int(3*M/4)]))) + [-1]\n",
    "                  }\n",
    "\n",
    "best_F, best_NT, best_score = None, None, -1\n",
    "all_metrics_CV = []\n",
    "\n",
    "for F_ in CV_dict_params['F']:\n",
    "    for NT_ in CV_dict_params['NT']:\n",
    "        all_scores = np.zeros(5)\n",
    "        N = len(X_trn)\n",
    "        for run in range(5):\n",
    "            \n",
    "            ind_tst_ = np.full(len(X_trn), False)\n",
    "            ind_tst_[int(N*run/5):int(N*(run+1)/5)] = True\n",
    "            \n",
    "            X_trn_, y_trn_ = X_trn[ind_tst_], y_trn[ind_tst_]\n",
    "            X_tst_, y_tst_ = X_trn[~ind_tst_], y_trn[~ind_tst_]\n",
    "            \n",
    "            clf = DecisionForest_df(NT=NT_, F=F_)\n",
    "            clf.fit(X_trn_, y_trn_)\n",
    "            all_scores[run] = clf.score(X_tst_, y_tst_)\n",
    "            \n",
    "        score_ = all_scores.mean()\n",
    "        all_metrics_CV.append(score_)\n",
    "        if score_ > best_score:\n",
    "            best_F, best_NT, best_score = F_, NT_, score_\n",
    "        print(f'(F, NT) = {(F_, NT_)} \\t--> \\t F1-Score = {round(score_, 3)}')\n",
    "        \n",
    "all_metrics_CV = np.array(all_metrics_CV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best Parameters (F, NT): {(best_F, best_NT)}')\n",
    "best_DF = DecisionForest_df(NT=best_NT, F=best_F)\n",
    "best_DF.fit(X_trn, y_trn, verbose=3)\n",
    "print(np.flip(np.argsort(best_DF.importance)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trn_hat = best_DF.predict(X_trn)\n",
    "y_tst_hat = best_DF.predict(X_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy (test): {round(accuracy_score(y_tst.to_numpy(), y_tst_hat), 3)}')\n",
    "print(f'Precision (test): {round(precision_score(y_tst.to_numpy(), y_tst_hat), 3)}')\n",
    "print(f'Recall (test): {round(recall_score(y_tst.to_numpy(), y_tst_hat), 3)}')\n",
    "print(f'F1 Score (test): {round(f1_score(y_tst.to_numpy(), y_tst_hat), 3)}')\n",
    "print('-'*15)\n",
    "print(f'Accuracy (train): {round(accuracy_score(y_trn.to_numpy(), y_trn_hat), 3)}')\n",
    "print(f'Precision (train): {round(precision_score(y_trn.to_numpy(), y_trn_hat), 3)}')\n",
    "print(f'Recall (train): {round(recall_score(y_trn.to_numpy(), y_trn_hat), 3)}')\n",
    "print(f'F1 Score (train): {round(f1_score(y_trn.to_numpy(), y_trn_hat), 3)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_importances = []\n",
    "all_metrics = []\n",
    "\n",
    "for F_ in CV_dict_params['F']:\n",
    "    for NT_ in CV_dict_params['NT']:\n",
    "        clf = DecisionForest_df(NT=NT_, F=F_)\n",
    "        print(f'(F, NT) = {(F_, NT_)}')\n",
    "        clf.fit(X_trn, y_trn, verbose=3)\n",
    "        y_tst_hat = clf.predict(X_tst)   \n",
    "        \n",
    "        all_importances.append(np.flip(np.argsort(clf.importance)))     \n",
    "        \n",
    "        acc = accuracy_score(y_tst.to_numpy(), y_tst_hat)\n",
    "        prec = precision_score(y_tst.to_numpy(), y_tst_hat)\n",
    "        rec = recall_score(y_tst.to_numpy(), y_tst_hat)\n",
    "        f1_ = f1_score(y_tst.to_numpy(), y_tst_hat)\n",
    "           \n",
    "        print(f'Accuracy (test): {round(acc, 3)}')\n",
    "        print(f'Precision (test): {round(prec, 3)}')\n",
    "        print(f'Recall (test): {round(rec, 3)}')\n",
    "        print(f'F1 Score (test): {round(f1_, 3)}')\n",
    "        print('-'*15)\n",
    "        \n",
    "        all_metrics.append([acc, prec, rec, f1_])\n",
    "        \n",
    "all_importances = np.array(all_importances)\n",
    "all_metrics = np.array(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_metrics.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_metrics_CV.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.print_latex import print_table\n",
    "\n",
    "# print(\"IMPORTANCES\")\n",
    "# print_table(all_importances.T)\n",
    "# print(\"-\"*15)\n",
    "\n",
    "# print(\"ACC - PRECISION - RECALL - F1 (TEST)\")\n",
    "# print_table(all_metrics.T)\n",
    "# print(\"-\"*15)\n",
    "\n",
    "# print(\"F1 (CV)\")\n",
    "# print_table(all_metrics_CV)\n",
    "# print(\"-\"*15)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "66403a1342d2aa7accf4b5905b124973c3be20b2a1242511608c6e5eca88e39b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('mai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
